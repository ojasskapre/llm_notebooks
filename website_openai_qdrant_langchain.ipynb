{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objectives of this notebook are:\n",
    "\n",
    "1. Loading webpages from a specific domain.\n",
    "2. Embedding the webpage content using OpenAI models.\n",
    "3. Storing the resulting vectors in the Qdrant vector database.\n",
    "4. Retrieving documents similar to a given query.\n",
    "5. Generating output for the query based on the retrieved documents using OpenAI.\n",
    "\n",
    "<br>\n",
    "\n",
    "This is the basic workflow\n",
    "![workflow](images/website_qa.png)\n",
    "\n",
    "<br>\n",
    "Before running this notebook, make sure the Qdrant docker image is running\n",
    "\n",
    "1. Make sure that Docker daemon is installed and running:\n",
    "    ```\n",
    "    sudo docker info\n",
    "    ```\n",
    "2. Pull the image:\n",
    "    ```\n",
    "    docker pull qdrant/qdrant\n",
    "    ```\n",
    "3. Run the container\n",
    "    ```\n",
    "    docker run -p 6333:6333 \\\n",
    "        -v $(pwd)/path/to/data:/qdrant/storage \\\n",
    "        qdrant/qdrant\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below line of code to Install dependencies\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Get the absolute path of the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append(current_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from pprint import pprint\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webpagelinkminer import WebPageLinkExtractor\n",
    "\n",
    "# url = \"https://python.langchain.com/en/latest/\"\n",
    "# url = \"https://gpt-index.readthedocs.io/en/latest/\"\n",
    "# url = \"https://docs.sqlalchemy.org/en/20/\"\n",
    "url = \"https://next-auth.js.org/getting-started/introduction\"\n",
    "# url = \"https://flask.palletsprojects.com/en/2.3.x/\"\n",
    "# url = \"https://svelte.dev/docs\"\n",
    "# url = \"https://firebase.google.com/docs\"\n",
    "# url = \"https://www.mysqltutorial.org/\"\n",
    "# url = \"https://nextjs.org/docs\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract links from same domain using the WebpageLinkMiner library (https://github.com/ojasskapre/WebPageLinkMiner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = WebPageLinkExtractor(url, max_depth=1000, algorithm='dfs')\n",
    "extracted_urls = asyncio.run(extractor.get_links_async())\n",
    "\n",
    "print(f'Number of URLs extracted: {len(extracted_urls)}')\n",
    "pprint(extracted_urls[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all the webpage links using Langchain WebBaseLoader (https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/web_base.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(extracted_urls)\n",
    "loader.requests_per_second = 1\n",
    "docs = loader.aload()\n",
    "\n",
    "print(f'Number of documents loaded: {len(docs)}')\n",
    "pprint(docs[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tiktoken encoder which is used for OpenAI models along with the Langchain RecursiveCharacterTextSplitter (https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f'Number of texts split: {len(texts)}')\n",
    "pprint(texts[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings for the split text using OpenAI embedding models and storing them in Qdrant vector database (https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/qdrant.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "qdrant_url = \"http://localhost:6333/\"\n",
    "qdrant_port = 6333\n",
    "\n",
    "qdrant = Qdrant.from_documents(documents=texts,\n",
    "                               embedding=embeddings, \n",
    "                               url=qdrant_url, \n",
    "                               collection_name=\"langchain_documents\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the documents that may contain answer for the query using the qdrant similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "query = \"How to protect backend API route? Give me code for that.\"\n",
    "\n",
    "found_docs = qdrant.similarity_search(query)\n",
    "print(found_docs[0].page_content)\n",
    "print(found_docs[0].metadata['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Question Answer sources chain using Langchain (https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html) to generate the output for the given query using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chain = load_qa_with_sources_chain(llm=llm, chain_type=\"stuff\")\n",
    "results = chain.run(input_documents=found_docs, question=query)\n",
    "print(results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create the Retriever QA with sources using Langchain (https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa_with_sources.html) to generate the output for the given query using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(llm, chain_type=\"stuff\", retriever=qdrant.as_retriever())\n",
    "results = chain({\"question\": query}, return_only_outputs=True)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a summarization chain to summarize all the retrieved documents  (https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following content. \n",
    "If the content has a python code snippet then return the code along with the summary else mention 'No Code Found'\n",
    "\n",
    "Content: {text}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "# print(chain.prompt)\n",
    "\n",
    "results = chain.run(input_documents=found_docs, return_only_outputs=False)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
